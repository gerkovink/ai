@inproceedings{berthelot2023,
  TITLE = {{Estimating the environmental impact of Generative-AI services using an LCA-based methodology}},
  AUTHOR = {Berthelot, Adrien and Caron, Eddy and Jay, Mathilde and Lef{\`e}vre, Laurent},
  URL = {https://inria.hal.science/hal-04346102},
  BOOKTITLE = {{CIRP LCE 2024 - 31st Conference on Life Cycle Engineering}},
  ADDRESS = {Turin, Italy},
  PAGES = {1-10},
  YEAR = {2024},
  MONTH = Jun,
  KEYWORDS = {Generative AI ; Life Cycle Analysis ; Digital services ; Greenhouse Gas Emission ; Energy ; Methodology},
  PDF = {https://inria.hal.science/hal-04346102v2/file/Pre-Print_Estimating_LCE.pdf},
  HAL_ID = {hal-04346102},
  HAL_VERSION = {v2},
}

@software{vink2024open,
  author       = {Gerko Vink and
                  Duco Veen and
                  Hanne Oberman},
  title        = {{gerkovink/openeducationbook: Open Education in 
                   Higher Education}},
  month        = jan,
  year         = 2024,
  publisher    = {Zenodo},
  version      = {v0.9-alpha.1},
  doi          = {10.5281/zenodo.10594358},
  url          = {https://doi.org/10.5281/zenodo.10594358}
}

@article{kumar2023faculty,
  title={Faculty members’ use of artificial intelligence to grade student papers: a case of implications},
  author={Kumar, Rahul},
  journal={International Journal for Educational Integrity},
  volume={19},
  number={1},
  pages={9},
  year={2023},
  publisher={Springer}
}

@article{chan2023policy,
  title={A comprehensive AI policy education framework for university teaching and learning},
  author={Chan, Cecilia Ka Yuk},
  journal={International journal of educational technology in higher education},
  volume={20},
  number={1},
  pages={38},
  year={2023},
  publisher={Springer}
}

@inproceedings{chien2023,
author = {Chien, Andrew A and Lin, Liuzixuan and Nguyen, Hai and Rao, Varsha and Sharma, Tristan and Wijayawardana, Rajini},
title = {Reducing the Carbon Impact of Generative AI Inference (today and in 2035)},
year = {2023},
isbn = {9798400702426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3604930.3605705},
doi = {10.1145/3604930.3605705},
abstract = {Generative AI, exemplified in ChatGPT, Dall-E 2, and Stable Diffusion, are exciting new applications consuming growing quantities of computing. We study the compute, energy, and carbon impacts of generative AI inference. Using ChatGPT as an exemplar, we create a workload model and compare request direction approaches (Local, Balance, CarbonMin), assessing their power use and carbon impacts.Our workload model shows that for ChatGPT-like services, inference dominates emissions, in one year producing 25x the carbon-emissions of training GPT-3. The workload model characterizes user experience, and experiments show that carbon emissions-aware algorithms (CarbonMin) can both maintain user experience and reduce carbon emissions dramatically (35\%). We also consider a future scenario (2035 workload and power grids), and show that CarbonMin can reduce emissions by 56\%. In both cases, the key is intelligent direction of requests to locations with low-carbon power. Combined with hardware technology advances, CarbonMin can keep emissions increase to only 20\% compared to 2022 levels for 55x greater workload. Finally we consider datacenter headroom to increase effectiveness of shifting. With headroom, CarbonMin reduces 2035 emissions by 71\%.},
booktitle = {Proceedings of the 2nd Workshop on Sustainable Computer Systems},
articleno = {11},
numpages = {7},
keywords = {generative AI, sustainability, carbon emissions, large language models, geographic shifting},
location = {Boston, MA, USA},
series = {HotCarbon '23}
}

@inproceedings{baldassarre2023, author = {Baldassarre, Maria Teresa and Caivano, Danilo and Fernandez Nieto, Berenice and Gigante, Domenico and Ragone, Azzurra}, title = {The Social Impact of Generative AI: An Analysis on ChatGPT}, year = {2023}, isbn = {9798400701160}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3582515.3609555}, doi = {10.1145/3582515.3609555}, abstract = {In recent months, the impact of Artificial Intelligence (AI) on citizens’ lives has gained considerable public interest, driven by the emergence of Generative AI models, ChatGPT in particular. The rapid development of these models has sparked heated discussions regarding their benefits, limitations, and associated risks. Generative models hold immense promise across multiple domains, such as healthcare, finance, and education, to cite a few, presenting diverse practical applications. Nevertheless, concerns about potential adverse effects have elicited divergent perspectives, ranging from privacy risks to escalating social inequality. This paper adopts a methodology to delve into the societal implications of Generative AI tools, focusing primarily on the case of ChatGPT. It evaluates the potential impact on several social sectors and illustrates the findings of a comprehensive literature review of both positive and negative effects, emerging trends, and areas of opportunity of Generative AI models. This analysis aims to facilitate an in-depth discussion by providing insights that can inspire policy, regulation, and responsible development practices to foster a citizen-centric AI.}, booktitle = {Proceedings of the 2023 ACM Conference on Information Technology for Social Good}, pages = {363–373}, numpages = {11}, keywords = {Citizen-centric AI, Generative AI Social Impact, Trustable AI}, location = {Lisbon, Portugal}, series = {GoodIT '23} }

@article{alkaissi2023artificial,
  title={Artificial hallucinations in ChatGPT: implications in scientific writing},
  author={Alkaissi, Hussam and McFarlane, Samy I},
  journal={Cureus},
  volume={15},
  number={2},
  year={2023},
  publisher={Cureus}
}

@article{Ostergaard2023,
    author = {Østergaard, Søren Dinesen and Nielbo, Kristoffer Laigaard},
    title = "{False Responses From Artificial Intelligence Models Are Not Hallucinations}",
    journal = {Schizophrenia Bulletin},
    volume = {49},
    number = {5},
    pages = {1105-1107},
    year = {2023},
    month = {05},
    abstract = "{As recently highlighted in the New England Journal of Medicine,1,2 artificial intelligence (AI) has the potential to revolutionize the field of medicine. While AI undoubtedly represents a set of extremely powerful technologies, it is not infallible. Accordingly, in their illustrative paper on potential medical applications of the recently launched large language model GPT-4, Lee et al. point out that chatbot applications for this AI-driven large language model occasionally produce false responses and that “A false response by GPT-4 is sometimes referred to as a ‘hallucination,’.”1 Indeed, it has become standard in AI to refer to a response that is not justified by the training data as a hallucination.3 We find this terminology to be problematic for the following 2 reasons:}",
    issn = {0586-7614},
    doi = {10.1093/schbul/sbad068},
    url = {https://doi.org/10.1093/schbul/sbad068},
    eprint = {https://academic.oup.com/schizophreniabulletin/article-pdf/49/5/1105/51375568/sbad068.pdf},
}




@article{athaluri2023exploring,
  title={Exploring the boundaries of reality: investigating the phenomenon of artificial intelligence hallucination in scientific writing through ChatGPT references},
  author={Athaluri, Sai Anirudh and Manthena, Sandeep Varma and Kesapragada, VSR Krishna Manoj and Yarlagadda, Vineel and Dave, Tirth and Duddumpudi, Rama Tulasi Siri},
  journal={Cureus},
  volume={15},
  number={4},
  year={2023},
  publisher={Cureus}
}

@article{ziwei, author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale}, title = {Survey of Hallucination in Natural Language Generation}, year = {2023}, issue_date = {December 2023}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {55}, number = {12}, issn = {0360-0300}, url = {https://doi.org/10.1145/3571730}, doi = {10.1145/3571730}, abstract = {Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation, and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before.In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions, and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.}, journal = {ACM Comput. Surv.}, month = {mar}, articleno = {248}, numpages = {38}, keywords = {consistency in NLG, factuality in NLG, faithfulness in NLG, extrinsic hallucination, intrinsic hallucination, Hallucination} }

@article{vaswani,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@book{duin2021writing,
  title={Writing futures: Collaborative, algorithmic, autonomous},
  author={Duin, Ann Hill and Pedersen, Isabel},
  year={2021},
  publisher={Springer}
}

@article{bockting2023,
author = {Bockting, Claudi and Dis, Eva A and Rooij, Robert and Zuidema, Willem and Bollen, Johan},
year = {2023},
month = {10},
pages = {693-696},
title = {Living guidelines for generative AI — why scientists must oversee its use},
volume = {622},
journal = {Nature},
doi = {10.1038/d41586-023-03266-1}
}

@article{REF,
  title={THERE IS A MISSING REFERENCE},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@incollection{turing,
    author = {Turing, Alan},
    isbn = {9780198250791},
    title = "{Lecture on the Automatic Computing Engine (1947)}",
    booktitle = "{The Essential Turing}",
    publisher = {Oxford University Press},
    year = {2004},
    month = {09},
    abstract = "{On 8 December 1943 the world’s first large-scale special-purpose electronic digital computer—‘Colossus’, as it became known—went into operation at the Government Code and Cypher School (see ‘Computable Numbers: A Guide’, ‘Enigma’, and the introduction to Chapter 4). Colossus was built by Thomas H. Flowers and his team of engineers at the Post Office Research Station in Doll is Hill, London. Until relatively recently, few had any idea that electronic digital computation was used successfully during the Second World War, since those who built and worked with Colossus were prohibited by the Official Secrets Act from sharing their knowledge. Colossus contained approximately the same number of electronic valves (vacuum tubes) as von Neumann’s IAS computer, built at the Princeton Institute of Advanced Study and dedicated in 1952. The IAS computer was forerunner of the IBM 701, the company’s first mass-produced stored-programme electronic computer (1953). The first Colossus had 1,600 electronic valves and Colossus II, installed in mid-1944, 2,400, while the IAS computer had 2,600. Colossus lacked two important features of modern computers. First, it had no internally stored programmes (see ‘Computable Numbers: A Guide’). To set up Colossus for a new task, the operators had to alter the machine’s physical wiring, using plugs and switches. Second, Colossus was not a general-purpose machine, being designed for a specific cryptanalytic task (involving only logical operations and counting). Nevertheless, Flowers had established decisively and for the first time that large-scale electronic computing machinery was practicable. The implication of Flowers’s racks of electronic equipment would have been obvious to Turing. Once Turing had seen Colossus it was, Flowers said, just a matter of Turing’s waiting to see what opportunity might arise to put the idea of his universal computing machine into practice. Precisely such an opportunity fell into Turing’s lap in 1945, when John Womersley invited him to join the Mathematics Division of the National Physical Laboratory (NPL) at Teddington in London, in order to design and develop an electronic stored-programme digital computer—a concrete form of the universal Turing machine of 1936.}",
    doi = {10.1093/oso/9780198250791.003.0015},
    url = {https://doi.org/10.1093/oso/9780198250791.003.0015},
    eprint = {https://academic.oup.com/book/0/chapter/355745788/chapter-pdf/43816995/isbn-9780198250791-book-part-15.pdf},
}

@article{eliza, author = {Weizenbaum, Joseph}, title = {ELIZA—a computer program for the study of natural language communication between man and machine}, year = {1966}, issue_date = {Jan. 1966}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {9}, number = {1}, issn = {0001-0782}, url = {https://doi.org/10.1145/365153.365168}, doi = {10.1145/365153.365168}, journal = {Commun. ACM}, month = {jan}, pages = {36–45}, numpages = {10} }

